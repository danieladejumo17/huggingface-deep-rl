{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Hyperparameter Tuning with Optuna - Lunar Lander.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyM+m1/rRz8OonX8sn1uLC22"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["\n","### Installs and Imports"],"metadata":{"id":"fuBWQMa0jZD_"}},{"cell_type":"code","source":["!pip install -q gym[box2d]\n","!pip install -q stable-baselines3[extra]\n","!pip install -q sb3-contrib\n","!pip install -q optuna"],"metadata":{"id":"_McpGNJWe4GD","executionInfo":{"status":"ok","timestamp":1661531558279,"user_tz":-60,"elapsed":14710,"user":{"displayName":"Adejumo Daniel","userId":"02925977078148845759"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","execution_count":18,"metadata":{"id":"oueD3i7Bee-j","executionInfo":{"status":"ok","timestamp":1661531558286,"user_tz":-60,"elapsed":118,"user":{"displayName":"Adejumo Daniel","userId":"02925977078148845759"}}},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","\n","import gym\n","from stable_baselines3 import PPO, A2C\n","from stable_baselines3.common.env_util import make_vec_env\n","from stable_baselines3.common.evaluation import evaluate_policy"]},{"cell_type":"code","source":["import optuna\n","from optuna.pruners import MedianPruner\n","from optuna.samplers import TPESampler\n","from optuna.visualization import plot_optimization_history, plot_param_importances"],"metadata":{"id":"ZCnBrhnGhkE5","executionInfo":{"status":"ok","timestamp":1661531558292,"user_tz":-60,"elapsed":120,"user":{"displayName":"Adejumo Daniel","userId":"02925977078148845759"}}},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":["### Config"],"metadata":{"id":"84f46OJWjegT"}},{"cell_type":"code","source":["N_TRIALS = 20 # maximum number of trials\n","N_JOBS = 1 # number of jobs to run in parallel\n","N_STARTUP_TRIALS = 5 # do N_STARTUP_TRIALS random sampling\n","N_EVALUATIONS = 2 # number of evaluations to run during training\n","N_TIMESTEPS = int(2e4) # training budget\n","EVAL_FREQ = int(N_TIMESTEPS / N_EVALUATIONS)\n","\n","N_EVAL_ENVS = 5\n","N_EVAL_EPISODES = 10\n","TIMEOUT = int(60*15)  # 15 minutes\n","\n","ENV_ID = \"Pendulum-v1\"\n","\n","DEFAULT_HYPERPARAMS = {\n","    \"policy\": \"MlpPolicy\",\n","    \"env\": ENV_ID\n","}"],"metadata":{"id":"XVXqxvyyex4M","executionInfo":{"status":"ok","timestamp":1661531558296,"user_tz":-60,"elapsed":122,"user":{"displayName":"Adejumo Daniel","userId":"02925977078148845759"}}},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":["### Search Space"],"metadata":{"id":"lRQoXZlDjwva"}},{"cell_type":"code","source":["from typing import Any, Dict\n","\n","def sample_ppo_params(trial: optuna.Trial) -> Dict[str, Any]:\n","  \"\"\"\n","  Sampler for PPO parameters\n","\n","  :param trial: Optuna trial object\n","  :return: The sampled hyperparemters dictionary for a given trial\n","  \"\"\"\n","  gamma = 1 - trial.suggest_float(\"gamma_\", 1e-4, 0.1, log=True)\n","  lr = trial.suggest_float(\"lr\", 1e-5, 1, log=True)\n","  n_steps = 2 ** trial.suggest_int(\"exponent_n_steps\", 3, 12)\n","  gae_lambda = 1 - trial.suggest_float(\"gae_lambda_\", 1e-4, 0.01, log=True)\n","  max_grad_norm = trial.suggest_float(\"max_grad_norm\", 0.3, 5, log=True)\n","\n","  net_arch = trial.suggest_categorical(\"arch\", [\"small\", \"tiny\"])\n","  activation_fn = trial.suggest_categorical(\"activation_fn\", [\"tanh\", \"relu\"])\n","  normalize_advantage = trial.suggest_categorical(\"normalize_advantage\", [\"True\", \"False\"])\n","\n","  # Display true values\n","  trial.set_user_attr(\"gamma\", gamma)\n","  trial.set_user_attr(\"n_steps\", n_steps)\n","  trial.set_user_attr(\"gae_lambda\", gae_lambda)\n","\n","  # Set network architecture and activation function from categorical suggestion\n","  net_arch = [\n","      {\"pi\": [64], \"vf\": [64]}\n","      if net_arch == \"tiny\"\n","      else {\"pi\": [64, 64], \"vf\": [64, 64]}]\n","  activation_fn = {\"tanh\": nn.Tanh, \"relu\": nn.ReLU}[activation_fn]\n","  normalize_advantage = {\"True\": True, \"False\": False}[normalize_advantage]\n","\n","\n","  return {\"gamma\": gamma,\n","          \"learning_rate\": lr,\n","          \"n_steps\": n_steps,\n","          \"gae_lambda\": gae_lambda,\n","          \"normalize_advantage\": normalize_advantage,\n","          \"max_grad_norm\": max_grad_norm,\n","          \"policy_kwargs\": {\"net_arch\": net_arch,\n","                            \"activation_fn\": activation_fn}\n","          }"],"metadata":{"id":"5ml8IgtRjtaq","executionInfo":{"status":"ok","timestamp":1661531614023,"user_tz":-60,"elapsed":563,"user":{"displayName":"Adejumo Daniel","userId":"02925977078148845759"}}},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":["### Evaluation Callbak Class"],"metadata":{"id":"pwO1qYErpp3J"}},{"cell_type":"code","source":["from stable_baselines3.common.callbacks import EvalCallback\n","\n","class TrialEvalCallback(EvalCallback):\n","  \"\"\"\n","  Callback used for evaluating and reporting a trial\n","\n","  :param eval_env: Evaluation environment\n","  :param trial: Optuna trial object\n","  :param: n_eval_episodes: Number of episodes used to evalaute policy\n","  :param: eval_freq: Number of successive steps after which policy is evaluated \n","      during training\n","  :param: deterministic: whether the evaluation should use a deterministic or \n","      stochastic policy\n","  :param: verbose: verbosity\n","  \"\"\"\n","  def __init__(\n","      self,\n","      eval_env: gym.Env,\n","      trial: optuna.Trial,\n","      n_eval_episodes: int = 5,\n","      eval_freq: int = 10_000,\n","      deterministic: bool = True,\n","      verbose: int = 0):\n","    \n","    super().__init__(\n","        eval_env=eval_env,\n","        n_eval_episodes=n_eval_episodes,\n","        eval_freq=eval_freq,\n","        deterministic=deterministic,\n","        verbose=verbose)\n","    self.trial = trial\n","    self.is_pruned = False\n","    self.eval_idx = 0\n","\n","  def _on_step(self) -> bool:\n","    if self.eval_freq > 0 and self.n_calls % self.eval_freq == 0:\n","      # Evalaute the policy. Done in the parent class\n","      super()._on_step()\n","      self.eval_idx += 1\n","\n","      # Send report to Optuna\n","      self.trial.report(self.last_mean_reward, self.eval_idx)\n","\n","      # Prune trial if needed\n","      if self.trial.should_prune():\n","        self.is_pruned = True\n","        return False\n","      return True       "],"metadata":{"id":"ZNpmFfugpl1N","executionInfo":{"status":"ok","timestamp":1661531614648,"user_tz":-60,"elapsed":22,"user":{"displayName":"Adejumo Daniel","userId":"02925977078148845759"}}},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":["### Objective Function"],"metadata":{"id":"zVmzs7wuurrp"}},{"cell_type":"code","source":["def objective(trial: optuna.Trial) -> float:\n","  \"\"\"\n","  Objective function used by Optuna to evaluate one configuration (trial, i.e. \n","  a set of parameters)\n","\n","  Given a trial object, it will sample one set of hyperparameters, evaluate it, \n","  and report the result (mean episodic reward)\n","\n","  :param trial: Optuna trial object\n","  :return: Mean episodic reward after training \n","  \"\"\"\n","\n","  # Initialize with default hyperparameters\n","  kwargs = DEFAULT_HYPERPARAMS.copy()\n","\n","  # Update hyperparameters\n","  kwargs.update(sample_ppo_params(trial))\n","\n","  # Create the model\n","  model = PPO(**kwargs)\n","\n","  # Create the evaluation env\n","  eval_envs = make_vec_env(ENV_ID, n_envs=N_EVAL_ENVS)\n","\n","  # Create EvalCallback object\n","  # TrailEvalCallback(eval_env, trial, n_eval_episodes, eval_freq, deterministic, verbose)\n","  eval_callback = TrialEvalCallback(eval_envs, trial, N_EVAL_EPISODES, EVAL_FREQ, True, 1)\n","\n","  # Train the model\n","  nan_encountered = False\n","\n","  try:\n","    model.learn(N_TIMESTEPS, callback=eval_callback)\n","  except AssertionError as e:\n","    # Sometimes, random parameters can generate NaN\n","    print(e)\n","    nan_encountered = True\n","  finally:\n","    model.env.close()\n","    eval_envs.close()\n","\n","  # Tell the optimizer that the trial failed\n","  if nan_encountered: return float(\"nan\")\n","\n","  if eval_callback.is_pruned: raise optuna.exceptions.TrialPruned()\n","  return eval_callback.last_mean_reward"],"metadata":{"id":"DrE4bq6Ws6mM","executionInfo":{"status":"ok","timestamp":1661531614651,"user_tz":-60,"elapsed":23,"user":{"displayName":"Adejumo Daniel","userId":"02925977078148845759"}}},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":["### The Optimization Loop"],"metadata":{"id":"uQujOCadzHJg"}},{"cell_type":"code","source":["# Set PyTorch number of threads to 1 for a faster training\n","torch.set_num_threads(1)\n","\n","# Select the sampler. It can be RandomSampler, TPESampler, CMAES, ...\n","sampler = TPESampler(n_startup_trials=N_STARTUP_TRIALS)\n","\n","# Select the pruner. Do not prune before 1/3 of the maximum budget is used\n","# Do not prune before (N_EVALUATIONS // 3) evaluations\n","pruner = MedianPruner(n_startup_trials=N_STARTUP_TRIALS, n_warmup_steps = N_EVALUATIONS // 3)\n","\n","# Create the study and start the hyperparameter optimization\n","study = optuna.create_study(sampler=sampler, pruner=pruner, direction=\"maximize\")\n","\n","try:\n","  study.optimize(objective, n_trials=N_TRIALS, n_jobs=N_JOBS, timeout=TIMEOUT)\n","except KeyboardInterrupt:\n","  pass"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gwTicpgxzEU0","outputId":"265a826a-cd58-4691-94f3-e81542b51584"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2022-08-26 16:33:33,980]\u001b[0m A new study created in memory with name: no-name-6c26cc06-ff5f-4911-aa20-7ed9ef79ac1a\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Eval num_timesteps=10000, episode_reward=-1472.42 +/- 47.67\n","Episode length: 200.00 +/- 0.00\n","New best mean reward!\n","Eval num_timesteps=20000, episode_reward=-1482.44 +/- 62.53\n","Episode length: 200.00 +/- 0.00\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2022-08-26 16:34:04,322]\u001b[0m Trial 0 finished with value: -1482.4371759 and parameters: {'gamma_': 0.026007345873336345, 'lr': 0.6375169849173647, 'exponent_n_steps': 11, 'gae_lambda_': 0.0030884824359835045, 'max_grad_norm': 0.35024907628223534, 'arch': 'small', 'activation_fn': 'relu', 'normalize_advantage': 'False'}. Best is trial 0 with value: -1482.4371759.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Eval num_timesteps=10000, episode_reward=-1240.66 +/- 216.91\n","Episode length: 200.00 +/- 0.00\n","New best mean reward!\n","Eval num_timesteps=20000, episode_reward=-1393.92 +/- 167.77\n","Episode length: 200.00 +/- 0.00\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2022-08-26 16:34:34,315]\u001b[0m Trial 1 finished with value: -1393.9244159 and parameters: {'gamma_': 0.00021066629126039035, 'lr': 0.032030162656981206, 'exponent_n_steps': 9, 'gae_lambda_': 0.006180113265782246, 'max_grad_norm': 0.4509718097693385, 'arch': 'small', 'activation_fn': 'tanh', 'normalize_advantage': 'False'}. Best is trial 1 with value: -1393.9244159.\u001b[0m\n","/usr/local/lib/python3.7/dist-packages/stable_baselines3/ppo/ppo.py:147: UserWarning: You have specified a mini-batch size of 64, but because the `RolloutBuffer` is of size `n_steps * n_envs = 8`, after every 0 untruncated mini-batches, there will be a truncated mini-batch of size 8\n","We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n","Info: (n_steps=8 and n_envs=1)\n","  f\"You have specified a mini-batch size of {batch_size},\"\n"]},{"output_type":"stream","name":"stdout","text":["Eval num_timesteps=10000, episode_reward=-1565.03 +/- 230.10\n","Episode length: 200.00 +/- 0.00\n","New best mean reward!\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2022-08-26 16:36:22,440]\u001b[0m Trial 2 finished with value: -1826.0837007999999 and parameters: {'gamma_': 0.01975947390077405, 'lr': 0.00012784545854165776, 'exponent_n_steps': 3, 'gae_lambda_': 0.00816279726526012, 'max_grad_norm': 1.3922991445418573, 'arch': 'small', 'activation_fn': 'tanh', 'normalize_advantage': 'True'}. Best is trial 1 with value: -1393.9244159.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Eval num_timesteps=20000, episode_reward=-1826.08 +/- 83.36\n","Episode length: 200.00 +/- 0.00\n","Eval num_timesteps=10000, episode_reward=-1399.56 +/- 280.43\n","Episode length: 200.00 +/- 0.00\n","New best mean reward!\n","Eval num_timesteps=20000, episode_reward=-1194.49 +/- 312.98\n","Episode length: 200.00 +/- 0.00\n","New best mean reward!\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2022-08-26 16:36:49,143]\u001b[0m Trial 3 finished with value: -1194.4876064999999 and parameters: {'gamma_': 0.00015226974522460345, 'lr': 0.0007220584872409857, 'exponent_n_steps': 12, 'gae_lambda_': 0.0035188424608009207, 'max_grad_norm': 0.5136550074077985, 'arch': 'tiny', 'activation_fn': 'tanh', 'normalize_advantage': 'True'}. Best is trial 3 with value: -1194.4876064999999.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Eval num_timesteps=10000, episode_reward=-1322.85 +/- 199.42\n","Episode length: 200.00 +/- 0.00\n","New best mean reward!\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2022-08-26 16:37:15,980]\u001b[0m Trial 4 finished with value: -1185.0895925999998 and parameters: {'gamma_': 0.016122971918859936, 'lr': 0.0021961632439701515, 'exponent_n_steps': 7, 'gae_lambda_': 0.0023638868953019436, 'max_grad_norm': 2.2954013500921446, 'arch': 'tiny', 'activation_fn': 'relu', 'normalize_advantage': 'True'}. Best is trial 4 with value: -1185.0895925999998.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Eval num_timesteps=20000, episode_reward=-1185.09 +/- 215.86\n","Episode length: 200.00 +/- 0.00\n","New best mean reward!\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/stable_baselines3/ppo/ppo.py:147: UserWarning: You have specified a mini-batch size of 64, but because the `RolloutBuffer` is of size `n_steps * n_envs = 32`, after every 0 untruncated mini-batches, there will be a truncated mini-batch of size 32\n","We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n","Info: (n_steps=32 and n_envs=1)\n","  f\"You have specified a mini-batch size of {batch_size},\"\n"]},{"output_type":"stream","name":"stdout","text":["Eval num_timesteps=10000, episode_reward=-1336.92 +/- 153.39\n","Episode length: 200.00 +/- 0.00\n","New best mean reward!\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2022-08-26 16:37:51,742]\u001b[0m Trial 5 finished with value: -1442.7867286 and parameters: {'gamma_': 0.00450247368878552, 'lr': 0.007232685038656514, 'exponent_n_steps': 5, 'gae_lambda_': 0.0002964864266286789, 'max_grad_norm': 4.785209983163338, 'arch': 'tiny', 'activation_fn': 'relu', 'normalize_advantage': 'True'}. Best is trial 4 with value: -1185.0895925999998.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Eval num_timesteps=20000, episode_reward=-1442.79 +/- 77.27\n","Episode length: 200.00 +/- 0.00\n","Eval num_timesteps=10000, episode_reward=-1296.36 +/- 337.26\n","Episode length: 200.00 +/- 0.00\n","New best mean reward!\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2022-08-26 16:38:18,093]\u001b[0m Trial 6 finished with value: -1244.2103943000002 and parameters: {'gamma_': 0.09748535054077735, 'lr': 1.1127691511840384e-05, 'exponent_n_steps': 7, 'gae_lambda_': 0.0006944763479084139, 'max_grad_norm': 2.654287914590901, 'arch': 'tiny', 'activation_fn': 'relu', 'normalize_advantage': 'True'}. Best is trial 4 with value: -1185.0895925999998.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Eval num_timesteps=20000, episode_reward=-1244.21 +/- 280.26\n","Episode length: 200.00 +/- 0.00\n","New best mean reward!\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2022-08-26 16:38:31,125]\u001b[0m Trial 7 pruned. \u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Eval num_timesteps=10000, episode_reward=-1459.78 +/- 69.90\n","Episode length: 200.00 +/- 0.00\n","New best mean reward!\n","Eval num_timesteps=10000, episode_reward=-1322.80 +/- 208.35\n","Episode length: 200.00 +/- 0.00\n","New best mean reward!\n","Eval num_timesteps=20000, episode_reward=-1564.15 +/- 240.51\n","Episode length: 200.00 +/- 0.00\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2022-08-26 16:38:57,135]\u001b[0m Trial 8 finished with value: -1564.1465950000002 and parameters: {'gamma_': 0.0016181032722194701, 'lr': 0.0007487584388668523, 'exponent_n_steps': 9, 'gae_lambda_': 0.00011543897053967674, 'max_grad_norm': 0.8398350505146229, 'arch': 'tiny', 'activation_fn': 'relu', 'normalize_advantage': 'False'}. Best is trial 4 with value: -1185.0895925999998.\u001b[0m\n","/usr/local/lib/python3.7/dist-packages/stable_baselines3/ppo/ppo.py:147: UserWarning: You have specified a mini-batch size of 64, but because the `RolloutBuffer` is of size `n_steps * n_envs = 32`, after every 0 untruncated mini-batches, there will be a truncated mini-batch of size 32\n","We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n","Info: (n_steps=32 and n_envs=1)\n","  f\"You have specified a mini-batch size of {batch_size},\"\n","\u001b[32m[I 2022-08-26 16:39:15,043]\u001b[0m Trial 9 pruned. \u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Eval num_timesteps=10000, episode_reward=-1359.14 +/- 179.05\n","Episode length: 200.00 +/- 0.00\n","New best mean reward!\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/stable_baselines3/ppo/ppo.py:147: UserWarning: You have specified a mini-batch size of 64, but because the `RolloutBuffer` is of size `n_steps * n_envs = 32`, after every 0 untruncated mini-batches, there will be a truncated mini-batch of size 32\n","We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n","Info: (n_steps=32 and n_envs=1)\n","  f\"You have specified a mini-batch size of {batch_size},\"\n","\u001b[32m[I 2022-08-26 16:39:36,062]\u001b[0m Trial 10 pruned. \u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Eval num_timesteps=10000, episode_reward=-1397.38 +/- 141.89\n","Episode length: 200.00 +/- 0.00\n","New best mean reward!\n","Eval num_timesteps=10000, episode_reward=-1152.15 +/- 224.49\n","Episode length: 200.00 +/- 0.00\n","New best mean reward!\n","Eval num_timesteps=20000, episode_reward=-1167.25 +/- 332.09\n","Episode length: 200.00 +/- 0.00\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2022-08-26 16:40:02,407]\u001b[0m Trial 11 finished with value: -1167.2527505000003 and parameters: {'gamma_': 0.00013251576375382216, 'lr': 0.00048335560344894916, 'exponent_n_steps': 12, 'gae_lambda_': 0.0028907100040943983, 'max_grad_norm': 0.7418640502748501, 'arch': 'tiny', 'activation_fn': 'tanh', 'normalize_advantage': 'True'}. Best is trial 11 with value: -1167.2527505000003.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Eval num_timesteps=10000, episode_reward=-1280.63 +/- 162.75\n","Episode length: 200.00 +/- 0.00\n","New best mean reward!\n","Eval num_timesteps=20000, episode_reward=-1332.05 +/- 65.07\n","Episode length: 200.00 +/- 0.00\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2022-08-26 16:40:28,904]\u001b[0m Trial 12 finished with value: -1332.0499849 and parameters: {'gamma_': 0.0004950849377566995, 'lr': 0.0006210959529831925, 'exponent_n_steps': 9, 'gae_lambda_': 0.0020502410332880676, 'max_grad_norm': 0.8647754318890651, 'arch': 'tiny', 'activation_fn': 'tanh', 'normalize_advantage': 'True'}. Best is trial 11 with value: -1167.2527505000003.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Eval num_timesteps=10000, episode_reward=-1008.95 +/- 90.39\n","Episode length: 200.00 +/- 0.00\n","New best mean reward!\n","Eval num_timesteps=20000, episode_reward=-1272.30 +/- 242.86\n","Episode length: 200.00 +/- 0.00\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2022-08-26 16:40:55,315]\u001b[0m Trial 13 finished with value: -1272.2974849 and parameters: {'gamma_': 0.005483854743828143, 'lr': 0.0001645777555208797, 'exponent_n_steps': 10, 'gae_lambda_': 0.0005665372753554061, 'max_grad_norm': 1.0013622997785006, 'arch': 'tiny', 'activation_fn': 'tanh', 'normalize_advantage': 'True'}. Best is trial 11 with value: -1167.2527505000003.\u001b[0m\n"]},{"output_type":"stream","name":"stdout","text":["Eval num_timesteps=10000, episode_reward=-1185.77 +/- 276.30\n","Episode length: 200.00 +/- 0.00\n","New best mean reward!\n","Eval num_timesteps=20000, episode_reward=-1161.87 +/- 198.58\n","Episode length: 200.00 +/- 0.00\n","New best mean reward!\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2022-08-26 16:41:21,650]\u001b[0m Trial 14 finished with value: -1161.8690614 and parameters: {'gamma_': 0.0007298739365178624, 'lr': 0.01838256851153824, 'exponent_n_steps': 12, 'gae_lambda_': 0.005664490677963549, 'max_grad_norm': 0.749765913625302, 'arch': 'tiny', 'activation_fn': 'tanh', 'normalize_advantage': 'True'}. Best is trial 14 with value: -1161.8690614.\u001b[0m\n"]}]},{"cell_type":"code","source":["# Print Results\n","print(\"Number of finished trials\", len(study.trials))\n","\n","trial = study.best_trial\n","print(\"Best trial:\")\n","print(f\"  Mean Reward: {trial.value}\")\n","print(\"  Params: \")\n","for key, value in trial.params.items():\n","  print(f\"    {key}: {value}\")\n","print(\"  User Attributes:\")\n","for key, value in trial.user_attrs.items():\n","  print(f\"    {key}: {value}\")\n","\n","\n","# Write Report\n","study.trials_dataframe().to_csv(\"study_results_a2c_cartpole.csv\")\n","\n","fig1 = plot_optimization_history(study)\n","fig2 = plot_param_importances(study)\n","\n","fig1.show()\n","fig2.show()"],"metadata":{"id":"GjHdDLHlmdxU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Fl3Hb0kJARcw"},"execution_count":null,"outputs":[]}]}